{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA93WUy1zzWf"
      },
      "source": [
        "## Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ZMgCvSRFqxE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "tensorflow doesn't detects GPU: \n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5573861357600468499\n",
            "]\n",
            "\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "torch detects GPU:  True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "import torch \n",
        "import torch.jit\n",
        "import torch.nn as nn \n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.linear_model import \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(\"tensorflow doesn't detects GPU: \")\n",
        "print(device_lib.list_local_devices() )\n",
        "print(tf.test.gpu_device_name())\n",
        "print(tf.config.list_physical_devices())\n",
        "\n",
        "print(\"torch detects GPU: \" , torch.cuda.is_available())\n",
        "\n",
        "oversampler = RandomOverSampler(random_state=0)\n",
        "undersampler = ClusterCentroids(random_state=0)\n",
        "\n",
        "\n",
        "def get_score(df_true, df_submission):\n",
        "    for i in range(df_true.shape[1]):\n",
        "        print(\"score for col \",i,\": \" , metrics.roc_auc_score(df_true[:,i], df_submission[:,i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC3EQFi20buB"
      },
      "source": [
        "## Load training data from csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_orig shape:  (18995, 420)\n",
            "X_normalized shape:  (18995, 420)\n",
            "X_small shape:  (18995, 35)\n",
            "Y1 shape:  (18995, 10)\n",
            "Y2 shape:  (18995,)\n",
            "Y3 shape:  (18995, 4)\n"
          ]
        }
      ],
      "source": [
        "features_df_orig = pd.read_csv(\"train_features_fixed.csv\")\n",
        "features_df_normalized = pd.read_csv(\"train_features_normalized.csv\")\n",
        "labels_df = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "\n",
        "# We can use a different version of the training data for each task\n",
        "X_orig = np.array(features_df_orig.iloc[:, 2:].values,  dtype=float).reshape(227940//12,35*12)\n",
        "X_normalized = np.array(features_df_normalized.iloc[:, 2:].values,  dtype=float).reshape(227940//12,35*12)\n",
        "X_small = np.array(features_df_normalized.iloc[:, 2:].values,  dtype=float)[::12]\n",
        "\n",
        "\n",
        "#use train to validation ratio of 0.8\n",
        "X_orig_train = X_orig[:int(X_orig.shape[0]*0.8)]\n",
        "X_orig_valid = X_orig[int(X_orig.shape[0]*0.8):]\n",
        "X_normalized_train = X_normalized[:int(X_normalized.shape[0]*0.8)]\n",
        "X_normalized_valid = X_normalized[int(X_normalized.shape[0]*0.8):]\n",
        "X_small_train = X_small[:int(X_small.shape[0]*0.8)]\n",
        "X_small_valid = X_small[int(X_small.shape[0]*0.8):]\n",
        "\n",
        "Y1 = np.array(labels_df.iloc[:, 1:11].values,  dtype=float)\n",
        "Y1_train = Y1[:int(Y1.shape[0]*0.8)]\n",
        "Y1_valid = Y1[int(Y1.shape[0]*0.8):]\n",
        "\n",
        "Y2 = np.array(labels_df.iloc[:, 11].values,  dtype=float)\n",
        "Y2_train = Y2[:int(Y2.shape[0]*0.8)]\n",
        "Y2_valid = Y2[int(Y2.shape[0]*0.8):]\n",
        "\n",
        "Y3 = np.array(labels_df.iloc[:, 12:].values,  dtype=float)\n",
        "Y3_train = Y3[:int(Y3.shape[0]*0.8)]\n",
        "Y3_valid = Y3[int(Y3.shape[0]*0.8):]\n",
        "\n",
        "print(\"X_orig shape: \", X_orig.shape)\n",
        "print(\"X_normalized shape: \", X_normalized.shape)\n",
        "print(\"X_small shape: \", X_small.shape)\n",
        "print(\"Y1 shape: \", Y1.shape)\n",
        "print(\"Y2 shape: \", Y2.shape)\n",
        "print(\"Y3 shape: \", Y3.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7_9Klvq7MO"
      },
      "source": [
        "## TASK 1: ORDERING OF MEDICAL TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_normalized_undersampling, Y1_undersampling = undersampler.fit_resample(X_small_train, Y1_train)\n",
        "\n",
        "# print(Y1_undersampling[Y1_undersampling==1].shape)\n",
        "# print(Y1_undersampling[Y1_undersampling==0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRllo2HLfXiu"
      },
      "outputs": [],
      "source": [
        "# neural network\n",
        "\n",
        "# inputs_dim = X_normalized.shape[1]\n",
        "# outputs_dim = Y1.shape[1]\n",
        "\n",
        "# model1 = tf.keras.Sequential([\n",
        "#   tf.keras.layers.Dense(units=100, input_shape=[inputs_dim], activation='sigmoid'),\n",
        "#   tf.keras.layers.Dense(units=25, activation='sigmoid'),\n",
        "#   tf.keras.layers.Dense(units=outputs_dim)\n",
        "# ])\n",
        "\n",
        "# model1.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1), metrics=['mean_squared_error'])\n",
        "\n",
        "# history = model1.fit(X_normalized_train, Y1_train, epochs=10) # , verbose=False\n",
        "# print(\"Finished training the model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score for col  0 :  0.7575912287731612\n",
            "score for col  1 :  0.7084412675138481\n",
            "score for col  2 :  0.6640872208014811\n",
            "score for col  3 :  0.6641900210843836\n",
            "score for col  4 :  0.66605390206377\n",
            "score for col  5 :  0.6619687660010241\n",
            "score for col  6 :  0.7074789719626168\n",
            "score for col  7 :  0.7068474965671228\n",
            "score for col  8 :  0.703060414163833\n",
            "score for col  9 :  0.7938427266441539\n"
          ]
        }
      ],
      "source": [
        "#logistic regression\n",
        "models = []\n",
        "pred1_ = []\n",
        "\n",
        "for i in range(0, 10):\n",
        "    lr = LogisticRegression(max_iter=1000)\n",
        "    lr.fit(X_normalized_train, Y1_train[:, i])\n",
        "    Y_pred_lr = lr.predict_proba(X_normalized_valid)[:,1]\n",
        "    pred1_.append(Y_pred_lr)\n",
        "    models.append(lr)\n",
        "    print(\"score for col \",i,\": \" , metrics.roc_auc_score(Y1_valid[:,i], Y_pred_lr.reshape(-1,1)))\n",
        "\n",
        "pred1_ = np.array(pred1_).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score for col  0 :  0.7767917593577716\n",
            "score for col  1 :  0.6750534579667644\n",
            "score for col  2 :  0.5463701714496593\n",
            "score for col  3 :  0.5455364975298952\n",
            "score for col  4 :  0.5458959550715081\n",
            "score for col  5 :  0.7439994026284349\n",
            "score for col  6 :  0.39759151090342676\n",
            "score for col  7 :  0.6986340762041696\n",
            "score for col  8 :  0.6378072122335119\n",
            "score for col  9 :  0.6771260315415129\n"
          ]
        }
      ],
      "source": [
        "#random forest\n",
        "models = []\n",
        "pred1_ = []\n",
        "\n",
        "for i in range(0, 10):\n",
        "    rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=10)\n",
        "    rfc.fit(X_normalized_train, Y2_train)\n",
        "    Y_pred_lr = rfc.predict_proba(X_normalized_valid)[:,1]\n",
        "    pred1_.append(Y_pred_lr)\n",
        "    models.append(rfc)\n",
        "    print(\"score for col \",i,\": \" , metrics.roc_auc_score(Y1_valid[:,i], Y_pred_lr.reshape(-1,1)))\n",
        "\n",
        "pred1_ = np.array(pred1_).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TASK 1: EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score for col  0 :  0.7575912287731612\n",
            "score for col  1 :  0.7084412675138481\n",
            "score for col  2 :  0.6640872208014811\n",
            "score for col  3 :  0.6641900210843836\n",
            "score for col  4 :  0.66605390206377\n",
            "score for col  5 :  0.6619687660010241\n",
            "score for col  6 :  0.7074789719626168\n",
            "score for col  7 :  0.7068474965671228\n",
            "score for col  8 :  0.703060414163833\n",
            "score for col  9 :  0.7938427266441539\n"
          ]
        }
      ],
      "source": [
        "# pred1 = model1.predict(X_normalized_valid)\n",
        "# print(np.var(pred1))\n",
        "for i in range(Y1_valid.shape[1]):\n",
        "    print(\"score for col \",i,\": \" , metrics.roc_auc_score(Y1_valid[:,i], pred1_[:,i]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TASK 2: SEPSIS PREDICTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### replace nan cols with nearest neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_df_normalized_withnancols = pd.read_csv(\"train_features_normalized_withnancols.csv\")\n",
        "X_normalized_withnancols = np.array(features_df_normalized_withnancols.iloc[:, 2:].values,  dtype=float).reshape(227940//12,35*12)\n",
        "\n",
        "# use KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_normalized_imputer = imputer.fit_transform(X_normalized_withnancols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(886,)\n",
            "(886,)\n"
          ]
        }
      ],
      "source": [
        "# X_small_undersampling, Y2_undersampling = undersampler.fit_resample(X_small_train, Y2_train)\n",
        "\n",
        "# print(Y2_undersampling[Y2_undersampling==1].shape)\n",
        "# print(Y2_undersampling[Y2_undersampling==0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n",
              "                ('ridge',\n",
              "                 Ridge(alpha=0.01, fit_intercept=False, solver='svd'))])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "#gaussian kernel (pretty bad)\n",
        "# kernel = \"rbf\"\n",
        "# reg = np.power(10., -1.5)\n",
        "# bandwidth = np.power(10., -0.6)\n",
        "# gamma = np.power(10., -bandwidth)\n",
        "# coef0 = 0\n",
        "# tol = 1e-1\n",
        "\n",
        "# model2 = svm.SVC(kernel=kernel, C=np.power(10., -reg), gamma=gamma, coef0=coef0, tol=tol, random_state=10)\n",
        "            \n",
        "# model2.fit(X_small_undersampling[:10000], Y2_undersampling[:10000])\n",
        "\n",
        "\n",
        "# polynomial kernel\n",
        "# degree = 3\n",
        "# l2_coef = 0.01\n",
        "\n",
        "# model2 = make_pipeline(\n",
        "#     PolynomialFeatures(degree),\n",
        "#     Ridge(alpha=l2_coef, fit_intercept=False, solver=\"svd\"))\n",
        "    \n",
        "# model2.fit(X_small_undersampling[:1000], Y2_undersampling[:1000])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### logistic regression attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(886,)\n",
            "(14310,)\n"
          ]
        }
      ],
      "source": [
        "# X_normalized_train_resampled, Y2_train_resampled = oversampler.fit_resample(X_normalized_train, Y2_train)\n",
        "# takes ages and makes it perform worse anyways\n",
        "# X_normalized_train_resampled, Y2_train_resampled = undersampler.fit_resample(X_normalized_train, Y2_train)\n",
        "# X_normalized_train_resampled = X_normalized_train\n",
        "# Y2_train_resampled = Y2_train\n",
        "\n",
        "# print(Y2_train_resampled[Y2_train_resampled==1].shape)\n",
        "# print(Y2_train_resampled[Y2_train_resampled==0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_normalized_imputer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-100-4b5a2e119c92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized_imputer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'X_normalized_imputer' is not defined"
          ]
        }
      ],
      "source": [
        "#logistic regression\n",
        "model2 = LogisticRegression(max_iter=3000)\n",
        "model2.fit(X_normalized_imputer, Y2_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=5, random_state=10)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#random forest\n",
        "model2 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=10)\n",
        "model2.fit(X_normalized_imputer, Y2_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdaBoostClassifier(n_estimators=100, random_state=10)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#ada boost\n",
        "model2 = AdaBoostClassifier(n_estimators=100, random_state=10)\n",
        "model2.fit(X_normalized_imputer, Y2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TASK 2: EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.48974285]\n",
            " [0.49146503]\n",
            " [0.49435812]\n",
            " ...\n",
            " [0.49288603]\n",
            " [0.49208397]\n",
            " [0.48865027]]\n",
            "(32,)\n",
            "(3767,)\n",
            "0.0001317494230267767\n",
            "(3799, 1)\n",
            "(3799,)\n",
            "(202,)\n",
            "(3597,)\n",
            "score :  0.6583821226159312\n"
          ]
        }
      ],
      "source": [
        "# pred2 = model2.predict(X_small_valid)\n",
        "# pred2 = (pred2+1)/2\n",
        "# np.clip(pred2, 0, 1, out=pred2)\n",
        "pred2 = model2.predict_proba(X_normalized_valid)[:,1].reshape(-1,1)\n",
        "print(pred2)\n",
        "print(pred2[pred2>0.5].shape)\n",
        "print(pred2[pred2<0.5].shape)\n",
        "print(np.var(pred2))\n",
        "print(pred2.shape)\n",
        "print(Y2_valid.shape)\n",
        "print(Y2_valid[Y2_valid==1].shape)\n",
        "print(Y2_valid[Y2_valid==0].shape)\n",
        "\n",
        "print(\"score : \" , metrics.roc_auc_score(Y2_valid, pred2.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TASK 3: KEYS VITALS SIGNS PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "475/475 [==============================] - 0s 626us/step - loss: 144.5326 - mean_squared_error: 144.5326\n",
            "Epoch 2/20\n",
            "475/475 [==============================] - 0s 598us/step - loss: 53.3450 - mean_squared_error: 53.3450\n",
            "Epoch 3/20\n",
            "475/475 [==============================] - 0s 611us/step - loss: 49.1719 - mean_squared_error: 49.1719\n",
            "Epoch 4/20\n",
            "475/475 [==============================] - 0s 588us/step - loss: 51.3503 - mean_squared_error: 51.3503\n",
            "Epoch 5/20\n",
            "475/475 [==============================] - 0s 590us/step - loss: 49.5765 - mean_squared_error: 49.5765\n",
            "Epoch 6/20\n",
            "475/475 [==============================] - 0s 584us/step - loss: 46.6667 - mean_squared_error: 46.6667\n",
            "Epoch 7/20\n",
            "475/475 [==============================] - 0s 577us/step - loss: 49.3449 - mean_squared_error: 49.3449\n",
            "Epoch 8/20\n",
            "475/475 [==============================] - 0s 609us/step - loss: 44.7133 - mean_squared_error: 44.7133\n",
            "Epoch 9/20\n",
            "475/475 [==============================] - 0s 622us/step - loss: 47.0967 - mean_squared_error: 47.0967\n",
            "Epoch 10/20\n",
            "475/475 [==============================] - 0s 600us/step - loss: 46.9750 - mean_squared_error: 46.9750\n",
            "Epoch 11/20\n",
            "475/475 [==============================] - 0s 563us/step - loss: 47.3149 - mean_squared_error: 47.31490s - loss: 46.2493 - mean_squared_error: 4\n",
            "Epoch 12/20\n",
            "475/475 [==============================] - 0s 575us/step - loss: 47.0130 - mean_squared_error: 47.0130\n",
            "Epoch 13/20\n",
            "475/475 [==============================] - 0s 582us/step - loss: 46.6623 - mean_squared_error: 46.6623\n",
            "Epoch 14/20\n",
            "475/475 [==============================] - 0s 575us/step - loss: 45.0045 - mean_squared_error: 45.0045\n",
            "Epoch 15/20\n",
            "475/475 [==============================] - 0s 573us/step - loss: 45.3847 - mean_squared_error: 45.3847\n",
            "Epoch 16/20\n",
            "475/475 [==============================] - 0s 617us/step - loss: 46.3263 - mean_squared_error: 46.3263\n",
            "Epoch 17/20\n",
            "475/475 [==============================] - 0s 601us/step - loss: 48.5288 - mean_squared_error: 48.5288\n",
            "Epoch 18/20\n",
            "475/475 [==============================] - 0s 571us/step - loss: 45.3380 - mean_squared_error: 45.3380\n",
            "Epoch 19/20\n",
            "475/475 [==============================] - 0s 643us/step - loss: 47.2230 - mean_squared_error: 47.2230\n",
            "Epoch 20/20\n",
            "475/475 [==============================] - 0s 826us/step - loss: 44.3855 - mean_squared_error: 44.3855\n",
            "Finished training the model\n"
          ]
        }
      ],
      "source": [
        "inputs_dim = X_orig.shape[1]\n",
        "outputs_dim = Y3.shape[1]\n",
        "\n",
        "model3 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(units=100, input_shape=[inputs_dim], activation='relu'),\n",
        "  tf.keras.layers.Dense(units=25, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=outputs_dim)\n",
        "])\n",
        "\n",
        "model3.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1), metrics=['mean_squared_error'])\n",
        "\n",
        "history = model3.fit(X_normalized_train, Y3_train, epochs=20) # , verbose=False\n",
        "print(\"Finished training the model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TASK 3: EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score for col  0 :  0.5163050146147022\n",
            "score for col  1 :  0.7952272827908078\n",
            "score for col  2 :  0.5\n",
            "score for col  3 :  0.8097082912806599\n"
          ]
        }
      ],
      "source": [
        "pred3 = model3.predict(X_normalized_valid)\n",
        "for i in range(Y3_valid.shape[1]):\n",
        "    print(\"score for col \",i,\": \" , 0.5 + 0.5 * np.maximum(0, metrics.r2_score(Y3_valid[:,i], pred3[:,i])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(151968, 37)\n",
            "(151968, 37)\n",
            "(12664, 420)\n",
            "(12664, 35)\n",
            "(12657,)\n",
            "(7,)\n"
          ]
        }
      ],
      "source": [
        "print(pd.read_csv(\"test_features.csv\").shape)\n",
        "print(pd.read_csv(\"test_features_normalized.csv\").shape)\n",
        "test_features_df_normalized = pd.read_csv(\"test_features_normalized.csv\")\n",
        "\n",
        "test_X_normalized = np.array(test_features_df_normalized.iloc[:, 2:].values,  dtype=float).reshape(151968//12,35*12)\n",
        "test_X_small = np.array(test_features_df_normalized.iloc[:, 2:].values,  dtype=float)[::12]\n",
        "\n",
        "print(test_X_normalized.shape)\n",
        "print(test_X_small.shape)\n",
        "\n",
        "\n",
        "pred1 = []\n",
        "for i in range(0, 10):\n",
        "    Y_pred_lr = pred1.append(models[i].predict_proba(test_X_normalized)[:,1])\n",
        "\n",
        "\n",
        "pred1 = np.array(pred1).transpose()\n",
        "# pred1 = model1.predict(test_X_normalized)\n",
        "pred2 = model2.predict(test_X_normalized)\n",
        "# pred2 = model2.predict(test_X_small)\n",
        "# pred2 = (pred2+1)/2\n",
        "# np.clip(pred2, 0, 1, out=pred2)\n",
        "pred3 = model3.predict(test_X_normalized)\n",
        "\n",
        "print(pred2[pred2==0].shape)\n",
        "print(pred2[pred2>0.5].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12664, 16)\n",
            "(12664, 16)\n",
            "(12664, 10)\n",
            "(12664,)\n",
            "(12664, 4)\n"
          ]
        }
      ],
      "source": [
        "sample = pd.read_csv(\"sample.csv\")\n",
        "print(sample.shape)\n",
        "submit =  pd.DataFrame(np.zeros((12664,16)), columns=[sample.columns])\n",
        "print(submit.shape)\n",
        "print(pred1.shape)\n",
        "print(pred2.shape)\n",
        "print(pred3.shape)\n",
        "# print(submit)\n",
        "\n",
        "\n",
        "submit.iloc[:,0] = sample.iloc[:,0]\n",
        "submit.iloc[:,1:11] = pred1\n",
        "submit.iloc[:,11] = pred2\n",
        "submit.iloc[:,12:] = pred3\n",
        "\n",
        "submit.to_csv('submit.zip', index=False, float_format='%.3f', compression='zip')\n",
        "submit.to_csv('submit.csv', index=False, float_format='%.3f')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "l02c01_celsius_to_fahrenheit.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
