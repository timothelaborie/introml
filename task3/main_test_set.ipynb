{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from ImageDataGeneratorCustom import ImageDataGeneratorCustom\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "from tensorflow import debugging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " anchor (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 1024)              7231056   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,231,056\n",
      "Trainable params: 1,311,744\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "URL = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n",
    "\n",
    "# feature_extractor = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (224,224,3))\n",
    "# for layer in feature_extractor.layers:\n",
    "#   layer.trainable = False\n",
    "\n",
    "feature_extractor = hub.KerasLayer(URL, input_shape=(224, 224,3),trainable=False)\n",
    "\n",
    "\n",
    "embedding = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    tf.keras.layers.Dense(1024, activation='linear', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(224, 224,3))\n",
    "\n",
    "\n",
    "emb = embedding(anchor_input)\n",
    "\n",
    "model = Model(\n",
    "    inputs=anchor_input, outputs=emb\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(model)\n",
    "\n",
    "\n",
    "# model_path = \"./siamese_model\"\n",
    "model_path = \"./checkpoint-2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = 2\n",
    "# pred = model.predict([np.random.rand(imgs,224,224,3)])\n",
    "# # print(pred.shape)\n",
    "# print(pred)\n",
    "# model.build(input_shape=(None,224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1a7c293ab20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "siamese_model.load_weights(model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1024)\n"
     ]
    }
   ],
   "source": [
    "imgs = 2\n",
    "pred = model.predict(np.random.rand(imgs,224,224,3))\n",
    "print(pred.shape)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(10000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# dictionary with all the image names and their features\n",
    "features_dict = {}\n",
    "\n",
    "#images (warning this takes about 11GB of RAM)\n",
    "images = np.zeros((10000,224,224,3),dtype=np.float32)\n",
    "\n",
    "#load images\n",
    "line_id = 0\n",
    "for filename in os.listdir('food_224'):\n",
    "    img = np.array(Image.open('food_224/' + filename))/255\n",
    "    images[line_id] = img\n",
    "    line_id += 1\n",
    "    if line_id % 10000 == 0:\n",
    "        print(line_id)\n",
    "        break\n",
    "    \n",
    "print(images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting 100\n",
      "predicting 200\n",
      "predicting 300\n",
      "predicting 400\n",
      "predicting 500\n",
      "predicting 600\n",
      "predicting 700\n",
      "predicting 800\n",
      "predicting 900\n",
      "predicting 1000\n",
      "predicting 1100\n",
      "predicting 1200\n",
      "predicting 1300\n",
      "predicting 1400\n",
      "predicting 1500\n",
      "predicting 1600\n",
      "predicting 1700\n",
      "predicting 1800\n",
      "predicting 1900\n",
      "predicting 2000\n",
      "predicting 2100\n",
      "predicting 2200\n",
      "predicting 2300\n",
      "predicting 2400\n",
      "predicting 2500\n",
      "predicting 2600\n",
      "predicting 2700\n",
      "predicting 2800\n",
      "predicting 2900\n",
      "predicting 3000\n",
      "predicting 3100\n",
      "predicting 3200\n",
      "predicting 3300\n",
      "predicting 3400\n",
      "predicting 3500\n",
      "predicting 3600\n",
      "predicting 3700\n",
      "predicting 3800\n",
      "predicting 3900\n",
      "predicting 4000\n",
      "predicting 4100\n",
      "predicting 4200\n",
      "predicting 4300\n",
      "predicting 4400\n",
      "predicting 4500\n",
      "predicting 4600\n",
      "predicting 4700\n",
      "predicting 4800\n",
      "predicting 4900\n",
      "predicting 5000\n",
      "predicting 5100\n",
      "predicting 5200\n",
      "predicting 5300\n",
      "predicting 5400\n",
      "predicting 5500\n",
      "predicting 5600\n",
      "predicting 5700\n",
      "predicting 5800\n",
      "predicting 5900\n",
      "predicting 6000\n",
      "predicting 6100\n",
      "predicting 6200\n",
      "predicting 6300\n",
      "predicting 6400\n",
      "predicting 6500\n",
      "predicting 6600\n",
      "predicting 6700\n",
      "predicting 6800\n",
      "predicting 6900\n",
      "predicting 7000\n",
      "predicting 7100\n",
      "predicting 7200\n",
      "predicting 7300\n",
      "predicting 7400\n",
      "predicting 7500\n",
      "predicting 7600\n",
      "predicting 7700\n",
      "predicting 7800\n",
      "predicting 7900\n",
      "predicting 8000\n",
      "predicting 8100\n",
      "predicting 8200\n",
      "predicting 8300\n",
      "predicting 8400\n",
      "predicting 8500\n",
      "predicting 8600\n",
      "predicting 8700\n",
      "predicting 8800\n",
      "predicting 8900\n",
      "predicting 9000\n",
      "predicting 9100\n",
      "predicting 9200\n",
      "predicting 9300\n",
      "predicting 9400\n",
      "predicting 9500\n",
      "predicting 9600\n",
      "predicting 9700\n",
      "predicting 9800\n",
      "predicting 9900\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "\n",
    "\n",
    "# convert every image into features\n",
    "features = siamese_model.predict(images[0:size])\n",
    "for i in np.arange(size,10001-size,size):\n",
    "    print(\"predicting\",i)\n",
    "    f = siamese_model.predict(images[i:i+size])\n",
    "    features = np.concatenate([features, f])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "line_id = 0\n",
    "for filename in os.listdir('food_224'):\n",
    "    features_dict[filename.replace(\".jpg\",\"\")] = features[line_id]\n",
    "    line_id += 1\n",
    "    if line_id % 10000 == 0:\n",
    "        print(line_id)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving features\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "print(\"saving features\")\n",
    "\n",
    "# save the features dictionary to a file using pickle\n",
    "with open('features.pickle', 'wb') as handle:\n",
    "    pickle.dump(features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "print(features_dict['00001'].shape)\n",
    "# print(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features_dict = {}\n",
    "# load the features dictionary from the file\n",
    "with open('features.pickle', 'rb') as handle:\n",
    "    features_dict = pickle.load(handle)\n",
    "    \n",
    "print(features_dict['00001'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = []\n",
    "line_id = 0\n",
    "with open('test_triplets.txt', 'r') as f:\n",
    "    # go through the lines\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "        embedding1 = features_dict[line.split(' ')[0]]\n",
    "        embedding2 = features_dict[line.split(' ')[1]]\n",
    "        embedding3 = features_dict[line.split(' ')[2]]\n",
    "\n",
    "        \n",
    "        #compare distance between embeddings\n",
    "        distance1 = np.sqrt(np.sum((embedding1 - embedding2)**2))\n",
    "        distance2 = np.sqrt(np.sum((embedding1 - embedding3)**2))\n",
    "\n",
    "        pred.append(1 if distance1 < distance2 else 0)\n",
    "\n",
    "        line_id += 1\n",
    "        if line_id % 1000 == 0:\n",
    "            print(line_id)\n",
    "            # break\n",
    "\n",
    "pred = np.array(pred,dtype=int)\n",
    "#save to file as int\n",
    "np.savetxt(\"submission.txt\", pred, delimiter=\"\\n\", fmt=\"%d\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
