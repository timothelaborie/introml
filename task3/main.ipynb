{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow_addons as tfa\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import *\n",
    "from DistanceLayer import DistanceLayer\n",
    "from SiameseModel import SiameseModel\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "batch_size = 19\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_triplets = np.loadtxt('./train_triplets_split.txt', dtype=str,delimiter = ' ')\n",
    "val_triplets = np.loadtxt('./val_triplets_split.txt', dtype=str,delimiter = ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  xl split:  1\n",
      "128/128 [==============================] - 3s 12ms/step - loss: 0.4607 - val_accuracy: 0.6379\n",
      "file:  xl split:  2\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.4613 - val_accuracy: 0.6330\n",
      "file:  xl split:  3\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4643 - val_accuracy: 0.6397\n",
      "file:  resnet split:  1\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.4609 - val_accuracy: 0.6421\n",
      "file:  resnet split:  2\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.4607 - val_accuracy: 0.6650\n",
      "file:  resnet split:  3\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.4627 - val_accuracy: 0.6547\n",
      "file:  clip split:  1\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4624 - val_accuracy: 0.6541\n",
      "file:  clip split:  2\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4684 - val_accuracy: 0.6520\n",
      "file:  clip split:  3\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4578 - val_accuracy: 0.6746\n",
      "file:  vit split:  1\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4578 - val_accuracy: 0.6140\n",
      "file:  vit split:  2\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4578 - val_accuracy: 0.6169\n",
      "file:  vit split:  3\n",
      "128/128 [==============================] - 2s 11ms/step - loss: 0.4662 - val_accuracy: 0.5983\n"
     ]
    }
   ],
   "source": [
    "for file in [\"xl\",\"resnet\",\"clip\",\"vit\"]:\n",
    "\n",
    "    features = {}\n",
    "    # file = \"clip\"\n",
    "    with open('features_' + file + '.pickle', 'rb') as handle:\n",
    "        features = pickle.load(handle)\n",
    "\n",
    "    feature_dim = features[\"02461\"].shape[0]\n",
    "\n",
    "    split_id = 1\n",
    "    n = np.array(train_triplets).shape[0]//3\n",
    "    for split in [train_triplets[i:i + n] for i in range(0, len(train_triplets)-1, n)]:\n",
    "\n",
    "        print(\"file: \", file, \"split: \", split_id)\n",
    "        embedding = tf.keras.Sequential([\n",
    "            Input(shape=(feature_dim,)),\n",
    "            # BatchNormalization(),\n",
    "            Dense(8192, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.001)),\n",
    "            # Dropout(0.6),\n",
    "            BatchNormalization(),\n",
    "            Dense(8192, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.001)),\n",
    "            # Dropout(0.6),\n",
    "            BatchNormalization(),\n",
    "            Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "        ])\n",
    "\n",
    "        anchor_input = layers.Input(name=\"anchor\", shape=(feature_dim))\n",
    "        positive_input = layers.Input(name=\"positive\", shape=(feature_dim))\n",
    "        negative_input = layers.Input(name=\"negative\", shape=(feature_dim))\n",
    "\n",
    "        distances = DistanceLayer()(\n",
    "            embedding(anchor_input),\n",
    "            embedding(positive_input),\n",
    "            embedding(negative_input),\n",
    "        )\n",
    "\n",
    "        model = Model(\n",
    "            inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    "        )\n",
    "\n",
    "        # model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #convert the features dictionary to a tensorflow hash table\n",
    "        keys = tf.constant([key for key in features.keys()])\n",
    "        values = tf.constant([val for val in features.values()])\n",
    "\n",
    "        table = tf.lookup.experimental.DenseHashTable(\n",
    "            value_dtype=tf.float32,\n",
    "            key_dtype=tf.string,\n",
    "            empty_key=\"empty_key\",\n",
    "            deleted_key=\"deleted_key\",\n",
    "            default_value=[-1]*feature_dim,\n",
    "            )\n",
    "\n",
    "        table.insert(keys, values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(split)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(val_triplets)\n",
    "        count = np.array(split).shape[0]\n",
    "        # print(count)\n",
    "        # print(np.array(val_triplets).shape[0])\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def load_image(inputs):\n",
    "            filenames = inputs\n",
    "            anchor = table[filenames[0]]\n",
    "            positive = table[filenames[1]]\n",
    "            negative = table[filenames[2]]\n",
    "            output = (anchor,positive,negative)\n",
    "            \n",
    "            return output\n",
    "\n",
    "        train_dataset = train_dataset.map(load_image).cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(20)\n",
    "        val_dataset = val_dataset.map(load_image).cache().batch(100).prefetch(20)\n",
    "\n",
    "        model = SiameseModel(model,margin=0.5,scale=64)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(0.000001))\n",
    "\n",
    "\n",
    "        train_steps_per_epoch = int(count/batch_size)\n",
    "        # checkpoint_filepath = './checkpoints/checkpoint-{epoch}/'\n",
    "        #save checkpoint after every epoch\n",
    "        # callback = ModelCheckpoint(\n",
    "        #     filepath=checkpoint_filepath,\n",
    "        #     # save_weights_only=True,\n",
    "        # )\n",
    "        callback = EarlyStopping(monitor='val_accuracy', patience=4,restore_best_weights=True)\n",
    "\n",
    "\n",
    "        model.fit(train_dataset, epochs=100,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[callback],validation_data=val_dataset)\n",
    "\n",
    "\n",
    "\n",
    "        #predict distances and save them to a file\n",
    "\n",
    "        dataset = np.loadtxt('./val_triplets_split_shuffled.txt', dtype=str,delimiter = ' ')\n",
    "        # print(dataset.shape)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "        dataset = dataset.map(load_image).batch(100)\n",
    "        left = []\n",
    "        right = []\n",
    "        for triplet in iter(dataset):\n",
    "            anchor = triplet[0]\n",
    "            positive = triplet[1]\n",
    "            negative = triplet[2]\n",
    "            distances = model.predict([anchor,positive,negative])\n",
    "            left_distances = distances[0]\n",
    "            right_distances = distances[1]\n",
    "            s = np.array(left_distances).shape[0]\n",
    "            # print(s)\n",
    "            for i in range(s):\n",
    "                left.append(distances[0][i])\n",
    "                right.append(distances[1][i])\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.read_csv(\"val_distances.csv\",index_col=0)\n",
    "        df['anchor_left_distance_'+file+'_'+str(split_id)] = left\n",
    "        df['anchor_right_distance_'+file+'_'+str(split_id)] = right\n",
    "        df.to_csv('val_distances.csv')\n",
    "\n",
    "        split_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a lot of the code comes from https://keras.io/examples/vision/siamese_network/\n",
    "and https://github.com/akarshzingade/image-similarity-deep-ranking/blob/master/deepRanking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this code was used to initialize the distances file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict distances and save them to a file\n",
    "# set_file = \"val\"\n",
    "# dataset = np.loadtxt('./'+set_file+'_triplets_split_shuffled.txt', dtype=str,delimiter = ' ')\n",
    "# print(dataset.shape)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "# dataset = dataset.map(load_image).batch(100)\n",
    "# data = []\n",
    "# for triplet in iter(dataset):\n",
    "#     anchor = triplet[0]\n",
    "#     positive = triplet[1]\n",
    "#     negative = triplet[2]\n",
    "#     distances = model.predict([anchor,positive,negative])\n",
    "#     s = np.array(distances).shape[1]\n",
    "#     for i in range(s):\n",
    "#         data.append([distances[0][i],distances[1][i]])\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data=data,columns=['anchor_left_distance_'+file,'anchor_right_distance_'+file])\n",
    "# df.to_csv(set_file+'_distances.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append other distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict distances and save them to a file\n",
    "# set_file = \"val\"\n",
    "# dataset = np.loadtxt('./'+set_file+'_triplets_split_shuffled.txt', dtype=str,delimiter = ' ')\n",
    "# print(dataset.shape)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "# dataset = dataset.map(load_image).batch(100)\n",
    "# left = []\n",
    "# right = []\n",
    "# for triplet in iter(dataset):\n",
    "#     anchor = triplet[0]\n",
    "#     positive = triplet[1]\n",
    "#     negative = triplet[2]\n",
    "#     distances = model.predict([anchor,positive,negative])\n",
    "#     left_distances = distances[0]\n",
    "#     right_distances = distances[1]\n",
    "#     s = np.array(left_distances).shape[0]\n",
    "#     # print(s)\n",
    "#     for i in range(s):\n",
    "#         left.append(distances[0][i])\n",
    "#         right.append(distances[1][i])\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv(set_file+\"_distances.csv\",index_col=0)\n",
    "# df['anchor_left_distance_'+file] = left\n",
    "# df['anchor_right_distance_'+file] = right\n",
    "# df.to_csv(set_file+'_distances.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using clip directly (sucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict distances and save them to a file\n",
    "# set_file = \"val\"\n",
    "# dataset = np.loadtxt('./'+set_file+'_triplets_split_shuffled.txt', dtype=str,delimiter = ' ')\n",
    "# print(dataset.shape)\n",
    "# left = []\n",
    "# right = []\n",
    "# for triplet in iter(dataset):\n",
    "#     anchor = triplet[0]\n",
    "#     positive = triplet[1]\n",
    "#     negative = triplet[2]\n",
    "#     # print(np.sum((features[anchor])**2))\n",
    "#     left.append(np.sum((features[anchor]-features[positive])**2))\n",
    "#     right.append(np.sum((features[anchor]-features[negative])**2))\n",
    "\n",
    "\n",
    "# df = pd.read_csv(set_file+\"_distances.csv\",index_col=0)\n",
    "# df['anchor_left_distance_clip2'] = left\n",
    "# df['anchor_right_distance_clip2'] = right\n",
    "# df.to_csv(set_file+'_distances.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
