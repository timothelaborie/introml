{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow_addons as tfa\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import *\n",
    "from DistanceLayer import DistanceLayer\n",
    "from SiameseModel import SiameseModel\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.2279101e-01 1.6950681e-03 2.0503240e+00 ... 1.2365297e+00 1.3910511e+00\n",
      " 9.2304796e-01]\n",
      "<class 'numpy.ndarray'>\n",
      "0.322791\n",
      "(2048,)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "file = \"resnet\"\n",
    "# load the features dictionary from the file\n",
    "with open('features_' + file + '.pickle', 'rb') as handle:\n",
    "    features = pickle.load(handle)\n",
    "\n",
    "feature_dim = features[\"02461\"].shape[0]\n",
    "\n",
    "print(features[\"02461\"])\n",
    "print(type(features[\"02461\"]))\n",
    "print(features[\"02461\"][0])\n",
    "print(features[\"02461\"].shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a lot of the code comes from https://keras.io/examples/vision/siamese_network/\n",
    "and https://github.com/akarshzingade/image-similarity-deep-ranking/blob/master/deepRanking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.Sequential([\n",
    "    Input(shape=(feature_dim,)),\n",
    "    # BatchNormalization(),\n",
    "    Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.001)),\n",
    "    # BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "    \n",
    "    Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.001)),\n",
    "    # BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "    \n",
    "    Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "])\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(feature_dim))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(feature_dim))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(feature_dim))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the features dictionary to a tensorflow hash table\n",
    "keys = tf.constant([key for key in features.keys()])\n",
    "values = tf.constant([val for val in features.values()])\n",
    "\n",
    "table = tf.lookup.experimental.DenseHashTable(\n",
    "    value_dtype=tf.float32,\n",
    "    key_dtype=tf.string,\n",
    "    empty_key=\"empty_key\",\n",
    "    deleted_key=\"deleted_key\",\n",
    "    default_value=[-1]*feature_dim,\n",
    "    )\n",
    "\n",
    "table.insert(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59520\n",
      "7471\n"
     ]
    }
   ],
   "source": [
    "train_triplets = np.loadtxt('./train_triplets.txt', dtype=str,delimiter = ' ')\n",
    "val_triplets = np.loadtxt('./val_triplets_split.txt', dtype=str,delimiter = ' ')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_triplets)\n",
    "count = np.array(train_triplets).shape[0]\n",
    "print(count)\n",
    "print(np.array(val_triplets).shape[0])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(inputs):\n",
    "    filenames = inputs\n",
    "    anchor = table[filenames[0]]\n",
    "    positive = table[filenames[1]]\n",
    "    negative = table[filenames[2]]\n",
    "    output = (anchor,positive,negative)\n",
    "    \n",
    "    return output\n",
    "\n",
    "train_dataset = train_dataset.map(load_image).cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(20)\n",
    "val_dataset = val_dataset.map(load_image).cache().batch(100).prefetch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseModel(model,margin=0.5,scale=64)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976/2976 [==============================] - 49s 16ms/step - loss: 0.2428 - val_accuracy: 0.8196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5943c6730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps_per_epoch = int(count/batch_size)\n",
    "# checkpoint_filepath = './checkpoints/checkpoint-{epoch}/'\n",
    "#save checkpoint after every epoch\n",
    "# callback = ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     # save_weights_only=True,\n",
    "# )\n",
    "# callback = EarlyStopping(monitor='val_accuracy', patience=4,restore_best_weights=True)\n",
    "\n",
    "\n",
    "model.fit(train_dataset, epochs=1,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,validation_data=val_dataset)#,callbacks=[callback]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this code was used to initialize the distances file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict distances and save them to a file\n",
    "# set_file = \"test\"\n",
    "# dataset = np.loadtxt('./'+set_file+'_triplets.txt', dtype=str,delimiter = ' ')\n",
    "# print(dataset.shape)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "# dataset = dataset.map(load_image).batch(100)\n",
    "# data = []\n",
    "# for triplet in iter(dataset):\n",
    "#     anchor = triplet[0]\n",
    "#     positive = triplet[1]\n",
    "#     negative = triplet[2]\n",
    "#     distances = model.predict([anchor,positive,negative])\n",
    "#     # print(np.array(distances[0]).shape)\n",
    "#     # print(np.array(distances[1]).shape)\n",
    "#     s = np.array(distances).shape[1]\n",
    "#     for i in range(s):\n",
    "#         data.append([distances[0][i],distances[1][i]])\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data=data,columns=['anchor_left_distance_'+file,'anchor_right_distance_'+file])\n",
    "# df.to_csv(set_file+'_distances.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append other distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59544, 3)\n"
     ]
    }
   ],
   "source": [
    "#predict distances and save them to a file\n",
    "set_file = \"test6\"\n",
    "dataset = np.loadtxt('./test_triplets.txt', dtype=str,delimiter = ' ')\n",
    "print(dataset.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "dataset = dataset.map(load_image).batch(100)\n",
    "left = []\n",
    "right = []\n",
    "for triplet in iter(dataset):\n",
    "    anchor = triplet[0]\n",
    "    positive = triplet[1]\n",
    "    negative = triplet[2]\n",
    "    distances = model.predict([anchor,positive,negative])\n",
    "    left_distances = distances[0]\n",
    "    right_distances = distances[1]\n",
    "    s = np.array(left_distances).shape[0]\n",
    "    # print(s)\n",
    "    for i in range(s):\n",
    "        left.append(distances[0][i])\n",
    "        right.append(distances[1][i])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(set_file+\"_distances.csv\",index_col=0)\n",
    "df['anchor_left_distance_'+file] = left\n",
    "df['anchor_right_distance_'+file] = right\n",
    "df.to_csv(set_file+'_distances.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using clip directly (sucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict distances and save them to a file\n",
    "# set_file = \"val\"\n",
    "# dataset = np.loadtxt('./'+set_file+'_triplets_split_shuffled.txt', dtype=str,delimiter = ' ')\n",
    "# print(dataset.shape)\n",
    "# left = []\n",
    "# right = []\n",
    "# for triplet in iter(dataset):\n",
    "#     anchor = triplet[0]\n",
    "#     positive = triplet[1]\n",
    "#     negative = triplet[2]\n",
    "#     # print(np.sum((features[anchor])**2))\n",
    "#     left.append(np.sum((features[anchor]-features[positive])**2))\n",
    "#     right.append(np.sum((features[anchor]-features[negative])**2))\n",
    "\n",
    "\n",
    "# df = pd.read_csv(set_file+\"_distances.csv\",index_col=0)\n",
    "# df['anchor_left_distance_clip2'] = left\n",
    "# df['anchor_right_distance_clip2'] = right\n",
    "# df.to_csv(set_file+'_distances.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
