{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import *\n",
    "from DistanceLayer import DistanceLayer\n",
    "from SiameseModel import SiameseModel\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# batch_size = 96\n",
    "# batch_size = 8\n",
    "# batch_size = 24\n",
    "batch_size = 19\n",
    "# batch_size = 3\n",
    "# batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "# load the features dictionary from the file\n",
    "with open('features_merged.pickle', 'rb') as handle:\n",
    "# with open('features_mn.pickle', 'rb') as handle:\n",
    "    features = pickle.load(handle)\n",
    "\n",
    "feature_dim = features[\"02461\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a lot of the code comes from https://keras.io/examples/vision/siamese_network/\n",
    "and https://github.com/akarshzingade/image-similarity-deep-ranking/blob/master/deepRanking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.Sequential([\n",
    "    Input(shape=(feature_dim,)),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
    "    Dropout(0.6),\n",
    "    Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
    "    Dropout(0.6),\n",
    "    Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "])\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(feature_dim))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(feature_dim))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(feature_dim))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the features dictionary to a tensorflow hash table\n",
    "keys = tf.constant([key for key in features.keys()])\n",
    "values = tf.constant([val for val in features.values()])\n",
    "\n",
    "table = tf.lookup.experimental.DenseHashTable(\n",
    "    value_dtype=tf.float32,\n",
    "    key_dtype=tf.string,\n",
    "    empty_key=\"empty_key\",\n",
    "    deleted_key=\"deleted_key\",\n",
    "    default_value=[-1]*feature_dim,\n",
    "    )\n",
    "\n",
    "table.insert(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7315\n"
     ]
    }
   ],
   "source": [
    "train_triplets = np.loadtxt('./train_triplets_split.txt', dtype=str,delimiter = ' ')\n",
    "val_triplets = np.loadtxt('./val_triplets_split.txt', dtype=str,delimiter = ' ')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_triplets)\n",
    "count = np.array(train_triplets).shape[0]\n",
    "print(count)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(inputs):\n",
    "    filenames = inputs\n",
    "    anchor = table[filenames[0]]\n",
    "    positive = table[filenames[1]]\n",
    "    negative = table[filenames[2]]\n",
    "    output = (anchor,positive,negative)\n",
    "    \n",
    "    return output\n",
    "\n",
    "train_dataset = train_dataset.map(load_image).cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(20)\n",
    "val_dataset = val_dataset.map(load_image).cache().batch(batch_size).prefetch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(model,margin=0.5)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is needed, otherwise nothing works\n",
    "pred = siamese_model.predict(next(iter(train_dataset)))\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "385/385 [==============================] - 18s 45ms/step - loss: 0.4380 - val_accuracy: 0.7187\n",
      "Epoch 2/100\n",
      "385/385 [==============================] - 18s 48ms/step - loss: 0.3543 - val_accuracy: 0.7372\n",
      "Epoch 3/100\n",
      "385/385 [==============================] - 19s 49ms/step - loss: 0.3083 - val_accuracy: 0.7451\n",
      "Epoch 4/100\n",
      "384/385 [============================>.] - ETA: 0s - loss: 0.2896"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = int(count/batch_size)\n",
    "checkpoint_filepath = './checkpoints/checkpoint-{epoch}/'\n",
    "#save checkpoint after every epoch\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    # save_weights_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "siamese_model.fit(train_dataset, epochs=100,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[model_checkpoint_callback],validation_data=val_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=\"model.png\",\n",
    "#     show_shapes=True,\n",
    "#     expand_nested=True,\n",
    "    \n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
