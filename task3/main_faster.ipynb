{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from ImageDataGeneratorCustom import ImageDataGeneratorCustom\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "from tensorflow import debugging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "model_path = \"./deep_ranking\"\n",
    "\n",
    "# batch_size = 96\n",
    "# batch_size = 8\n",
    "batch_size = 24\n",
    "# batch_size = 3\n",
    "# batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a lot of the code comes from https://keras.io/examples/vision/siamese_network/\n",
    "and https://github.com/akarshzingade/image-similarity-deep-ranking/blob/master/deepRanking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding = tf.keras.Sequential([\n",
    "\n",
    "    # GlobalAveragePooling2D(),\n",
    "\n",
    "    tf.keras.layers.Dense(1024, activation='linear'),\n",
    "\n",
    "\n",
    "    # tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    # tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "])\n",
    "\n",
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(1280))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(1280))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(1280))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.5094212 , 0.48491645], dtype=float32), array([0.46956447, 0.49695075], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "imgs = 2\n",
    "pred = model.predict((np.random.rand(imgs,1280),np.random.rand(imgs,1280),np.random.rand(imgs,1280)))\n",
    "# print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "# load the features dictionary from the file\n",
    "with open('features.pickle', 'rb') as handle:\n",
    "    features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the features dictionary to a tensorflow hash table\n",
    "keys = tf.constant([key for key in features.keys()])\n",
    "values = tf.constant([val for val in features.values()])\n",
    "\n",
    "table = tf.lookup.experimental.DenseHashTable(\n",
    "    value_dtype=tf.float32,\n",
    "    key_dtype=tf.string,\n",
    "    empty_key=\"empty_key\",\n",
    "    deleted_key=\"deleted_key\",\n",
    "    default_value=[-1]*1280,\n",
    "    )\n",
    "\n",
    "table.insert(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load strings from train_triplets.txt\n",
    "train_triplets = np.loadtxt('./train_triplets.txt', dtype=str,delimiter = ' ')\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(inputs):\n",
    "    filenames = inputs\n",
    "    # print(filenames)\n",
    "\n",
    "\n",
    "    # print(table[filenames[0]])\n",
    "    anchor = table[filenames[0]]\n",
    "    positive = table[filenames[1]]\n",
    "    negative = table[filenames[2]]\n",
    "\n",
    "    output = (anchor,positive,negative)\n",
    "    \n",
    "    return output\n",
    "\n",
    "dataset = dataset.map(load_image).cache().batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,p,n = np.loadtxt('./train_triplets.txt', dtype=str,delimiter = ' ',unpack=True)\n",
    "# print(a.shape)\n",
    "\n",
    "# train_triplets = ([features[s] for s in a], [features[s] for s in p], [features[s] for s in n])\n",
    "# print(len(train_triplets))\n",
    "# print(len(train_triplets[0]))\n",
    "# print(len(train_triplets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "\n",
    "# dataset = dataset.cache().batch(batch_size).prefetch(300*batch_size)#.shuffle(300*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "# def plotImages(images_arr):\n",
    "#     fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "#     axes = axes.flatten()\n",
    "#     for img, ax in zip(images_arr, axes):\n",
    "#         ax.imshow(img)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# augmented_images = [my_gen[0][0][0] for i in range(5)]\n",
    "# plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(model)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1.2785615, 1.6343156, 1.3816605, 1.6875057, 1.5985289, 1.2669227,\n",
      "       1.3876119, 1.3485388, 1.4234812, 1.6437916, 1.1710888, 1.5574226,\n",
      "       1.6527615, 0.9138279, 1.0480176, 1.4959127, 0.9701855, 1.486997 ,\n",
      "       1.4662466, 1.246716 , 1.6518309, 1.5352551, 1.3594271, 1.4621719,\n",
      "       1.5229247, 1.2890278, 1.5469978, 1.644982 , 1.0517783, 1.7285328,\n",
      "       1.5778354, 1.0766933], dtype=float32), array([1.1514217, 1.4325662, 1.2593817, 1.223447 , 1.6302562, 1.3890911,\n",
      "       1.405709 , 1.2670089, 1.2581236, 1.5187016, 1.364988 , 1.3034827,\n",
      "       1.564629 , 1.442989 , 1.6237535, 1.7120795, 1.7828588, 0.9394683,\n",
      "       1.5499583, 1.0942109, 1.5222197, 1.1909884, 1.2209762, 1.3563652,\n",
      "       1.5907545, 1.5285754, 1.1498975, 1.7516456, 1.4020392, 1.4894664,\n",
      "       1.086106 , 1.5870796], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#this is needed, otherwise nothing works\n",
    "pred = siamese_model.predict(next(iter(dataset)))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_gen.__getitem__(5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480/2480 [==============================] - 13s 5ms/step - loss: 0.4360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c42699b070>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps_per_epoch = int((59520)/batch_size)\n",
    "# train_steps_per_epoch = int((59520*5)/batch_size)\n",
    "\n",
    "# checkpoint_filepath = './checkpoint'\n",
    "checkpoint_filepath = './checkpoints/checkpoint-{epoch}/'\n",
    "# checkpoint_filepath = \"saved-model-{epoch}.hdf5\"\n",
    "# checkpoint_filepath = \"saved-model-{epoch}\"\n",
    "#save checkpoint after every epoch\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    # save_weights_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "siamese_model.fit(dataset, epochs=1,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[model_checkpoint_callback])\n",
    "# siamese_model.fit(train_triplets, epochs=2,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_path = \"siamese_model\"\n",
    "# tf.saved_model.save(siamese_model, model_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
