{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from ImageDataGeneratorCustom import ImageDataGeneratorCustom\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "from tensorflow import debugging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import *\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "model_path = \"./deep_ranking\"\n",
    "\n",
    "# batch_size = 96\n",
    "# batch_size = 8\n",
    "batch_size = 24\n",
    "# batch_size = 3\n",
    "# batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "# load the features dictionary from the file\n",
    "with open('features_merged.pickle', 'rb') as handle:\n",
    "    features = pickle.load(handle)\n",
    "\n",
    "feature_dim = features[\"02461\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a lot of the code comes from https://keras.io/examples/vision/siamese_network/\n",
    "and https://github.com/akarshzingade/image-similarity-deep-ranking/blob/master/deepRanking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dense(256, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dense(256, activation='linear'),\n",
    "\n",
    "    # GlobalAveragePooling2D(),\n",
    "\n",
    "    # tf.keras.layers.Dense(1024, activation='linear'),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(4096, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "\n",
    "    # tf.keras.layers.Dense(8192, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    # tf.keras.layers.Dense(8192, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    # tf.keras.layers.Dense(8192, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    # tf.keras.layers.Dense(8192, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    # tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "])\n",
    "\n",
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(feature_dim))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(feature_dim))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(feature_dim))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = 2\n",
    "# pred = model.predict((np.random.rand(imgs,1280),np.random.rand(imgs,1280),np.random.rand(imgs,1280)))\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the features dictionary to a tensorflow hash table\n",
    "keys = tf.constant([key for key in features.keys()])\n",
    "values = tf.constant([val for val in features.values()])\n",
    "\n",
    "table = tf.lookup.experimental.DenseHashTable(\n",
    "    value_dtype=tf.float32,\n",
    "    key_dtype=tf.string,\n",
    "    empty_key=\"empty_key\",\n",
    "    deleted_key=\"deleted_key\",\n",
    "    default_value=[-1]*feature_dim,\n",
    "    )\n",
    "\n",
    "table.insert(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load strings from train_triplets.txt\n",
    "train_triplets = np.loadtxt('./train_triplets.txt', dtype=str,delimiter = ' ')\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(inputs):\n",
    "    filenames = inputs\n",
    "    # print(filenames)\n",
    "\n",
    "\n",
    "    # print(table[filenames[0]])\n",
    "    anchor = table[filenames[0]]\n",
    "    positive = table[filenames[1]]\n",
    "    negative = table[filenames[2]]\n",
    "\n",
    "    output = (anchor,positive,negative)\n",
    "    \n",
    "    return output\n",
    "\n",
    "dataset = dataset.map(load_image).cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,p,n = np.loadtxt('./train_triplets.txt', dtype=str,delimiter = ' ',unpack=True)\n",
    "# print(a.shape)\n",
    "\n",
    "# train_triplets = ([features[s] for s in a], [features[s] for s in p], [features[s] for s in n])\n",
    "# print(len(train_triplets))\n",
    "# print(len(train_triplets[0]))\n",
    "# print(len(train_triplets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(train_triplets)\n",
    "\n",
    "# dataset = dataset.cache().batch(batch_size).prefetch(300*batch_size)#.shuffle(300*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "# def plotImages(images_arr):\n",
    "#     fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "#     axes = axes.flatten()\n",
    "#     for img, ax in zip(images_arr, axes):\n",
    "#         ax.imshow(img)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# augmented_images = [my_gen[0][0][0] for i in range(5)]\n",
    "# plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(model,margin=0.5)\n",
    "# siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "# siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0000005))\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.000001))\n",
    "\n",
    "\n",
    "# class MyExponentialDecay(tf.keras.optimizers.schedules.ExponentialDecay):\n",
    "#   def __call__(self, step):\n",
    "#     return 1e-2 * super().__call__(step)\n",
    "\n",
    "# initial_learning_rate = 1e-2\n",
    "# wd = MyExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=14,\n",
    "#     decay_rate=0.8,\n",
    "#     staircase=True)\n",
    "\n",
    "\n",
    "# step = tf.Variable(0, trainable=False)\n",
    "# schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [1e-0, 1e-1, 1e-2])\n",
    "# lr = 1e-1 * schedule(step)\n",
    "# # wd = lambda: 1e-4 * schedule(step)\n",
    "# opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "\n",
    "# siamese_model.compile(optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.516847  , 0.43181312, 0.4940071 , 0.5842757 , 0.5482377 ,\n",
      "       0.43574452, 0.55073047, 0.42874548, 0.46582067, 0.4362592 ,\n",
      "       0.38197672, 0.36572403, 0.48934537, 0.6045705 , 0.5729515 ,\n",
      "       0.42375416, 0.36645746, 0.45063758, 0.4059661 , 0.46973476,\n",
      "       0.6046382 , 0.5449496 , 0.45889103, 0.5293103 ], dtype=float32), array([0.45667267, 0.55163443, 0.46766412, 0.5917454 , 0.6091945 ,\n",
      "       0.6577747 , 0.3886032 , 0.5379775 , 0.6182839 , 0.583356  ,\n",
      "       0.50954795, 0.5639539 , 0.39135927, 0.49001554, 0.4421147 ,\n",
      "       0.35350835, 0.40926147, 0.38413885, 0.5227969 , 0.48677132,\n",
      "       0.43193632, 0.55536914, 0.42086077, 0.55242157], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#this is needed, otherwise nothing works\n",
    "pred = siamese_model.predict(next(iter(dataset)))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_gen.__getitem__(5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2480/2480 [==============================] - 49s 20ms/step - loss: 0.3320\n",
      "Epoch 2/100\n",
      "2480/2480 [==============================] - 43s 17ms/step - loss: 0.2782\n",
      "Epoch 3/100\n",
      "2480/2480 [==============================] - 43s 17ms/step - loss: 0.2635\n",
      "Epoch 4/100\n",
      "2480/2480 [==============================] - 43s 17ms/step - loss: 0.2526\n",
      "Epoch 5/100\n",
      "2480/2480 [==============================] - 43s 17ms/step - loss: 0.2433\n",
      "Epoch 6/100\n",
      "2480/2480 [==============================] - 43s 17ms/step - loss: 0.2350\n",
      "Epoch 7/100\n",
      "2480/2480 [==============================] - 45s 18ms/step - loss: 0.2273\n",
      "Epoch 8/100\n",
      "2480/2480 [==============================] - 47s 19ms/step - loss: 0.2201\n",
      "Epoch 9/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.2131\n",
      "Epoch 10/100\n",
      "2480/2480 [==============================] - 47s 19ms/step - loss: 0.2064\n",
      "Epoch 11/100\n",
      "2480/2480 [==============================] - 47s 19ms/step - loss: 0.1998\n",
      "Epoch 12/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1934\n",
      "Epoch 13/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1872\n",
      "Epoch 14/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1811\n",
      "Epoch 15/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1751\n",
      "Epoch 16/100\n",
      "2480/2480 [==============================] - 47s 19ms/step - loss: 0.1692\n",
      "Epoch 17/100\n",
      "2480/2480 [==============================] - 48s 20ms/step - loss: 0.1635\n",
      "Epoch 18/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1578\n",
      "Epoch 19/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1523\n",
      "Epoch 20/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1468\n",
      "Epoch 21/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1415\n",
      "Epoch 22/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1363\n",
      "Epoch 23/100\n",
      "2480/2480 [==============================] - 48s 19ms/step - loss: 0.1312\n",
      "Epoch 24/100\n",
      "2480/2480 [==============================] - 49s 20ms/step - loss: 0.1262\n",
      "Epoch 25/100\n",
      " 866/2480 [=========>....................] - ETA: 34s - loss: 0.1224"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = int((59520)/batch_size)\n",
    "# train_steps_per_epoch = int((59520*5)/batch_size)\n",
    "\n",
    "# checkpoint_filepath = './checkpoint'\n",
    "checkpoint_filepath = './checkpoints/checkpoint-{epoch}/'\n",
    "# checkpoint_filepath = \"saved-model-{epoch}.hdf5\"\n",
    "# checkpoint_filepath = \"saved-model-{epoch}\"\n",
    "#save checkpoint after every epoch\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    # save_weights_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "siamese_model.fit(dataset, epochs=100,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[model_checkpoint_callback])\n",
    "# siamese_model.fit(train_triplets, epochs=2,steps_per_epoch=train_steps_per_epoch,batch_size=batch_size,callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=\"model.png\",\n",
    "#     show_shapes=True,\n",
    "#     expand_nested=True,\n",
    "    \n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
