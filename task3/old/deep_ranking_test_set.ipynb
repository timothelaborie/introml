{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from ImageDataGeneratorCustom import ImageDataGeneratorCustom\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convnet_model_():\n",
    "#     vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "#     vgg.trainable = False\n",
    "#     model = tf.keras.Sequential([\n",
    "#         #VGG16\n",
    "#         vgg,\n",
    "#         #GlobalAveragePooling2D\n",
    "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#         #Dense 4096 relu\n",
    "#         tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#         # #Dropout 0.5\n",
    "#         # tf.keras.layers.Dropout(0.5),\n",
    "#         # #Dense 4096 relu\n",
    "#         # tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#         #Dropout 0.5\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         #L2 normalization\n",
    "#         tf.keras.layers.Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "#     ])\n",
    "\n",
    "#     return model\n",
    "\n",
    "def convnet_model_():\n",
    "    URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "    feature_extractor = hub.KerasLayer(URL, input_shape=(224, 224,3))\n",
    "    feature_extractor.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extractor,\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        #L2 normalization\n",
    "        tf.keras.layers.Lambda(lambda  x: K.l2_normalize(x,axis=1))\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def deep_rank_model():\n",
    " \n",
    "    convnet_model = convnet_model_()\n",
    "    first_input = Input(shape=(224,224,3))\n",
    "    first_conv = Conv2D(96, kernel_size=(8, 8),strides=(16,16), padding='same')(first_input)\n",
    "    first_max = MaxPool2D(pool_size=(3,3),strides = (4,4),padding='same')(first_conv)\n",
    "    first_max = Flatten()(first_max)\n",
    "    first_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(first_max)\n",
    "\n",
    "    second_input = Input(shape=(224,224,3))\n",
    "    second_conv = Conv2D(96, kernel_size=(8, 8),strides=(32,32), padding='same')(second_input)\n",
    "    second_max = MaxPool2D(pool_size=(7,7),strides = (2,2),padding='same')(second_conv)\n",
    "    second_max = Flatten()(second_max)\n",
    "    second_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(second_max)\n",
    "\n",
    "    merge_one = concatenate([first_max, second_max])\n",
    "\n",
    "    merge_two = concatenate([merge_one, convnet_model.output])\n",
    "    emb = Dense(2048)(merge_two)\n",
    "    l2_norm_final = Lambda(lambda  x: K.l2_normalize(x,axis=1))(emb)\n",
    "\n",
    "    final_model = Model(inputs=[first_input, second_input, convnet_model.input], outputs=l2_norm_final)\n",
    "\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 96)   18528       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 96)     18528       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " keras_layer_input (InputLayer)  [(None, 224, 224, 3  0          []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 4, 4, 96)     0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 4, 4, 96)    0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       (None, 1280)         2257984     ['keras_layer_input[0][0]']      \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1536)         0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1536)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096)         5246976     ['keras_layer[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1536)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1536)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3072)         0           ['lambda_1[0][0]',               \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 4096)         0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 7168)         0           ['concatenate[0][0]',            \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2048)         14682112    ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 2048)         0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,224,128\n",
      "Trainable params: 19,966,144\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_rank_model = deep_rank_model()\n",
    "\n",
    "# for layer in deep_rank_model.layers:\n",
    "#     print (layer.name, layer.output_shape)\n",
    "\n",
    "model_path = \"./deep_ranking\"\n",
    "\n",
    "deep_rank_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x231ae3c22e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.saved_model.load(deep_rank_model, \"./deep_ranking\")\n",
    "deep_rank_model.load_weights('deepranking')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048,)\n",
      "(2048,)\n",
      "(2048,)\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "image1 = np.array(Image.open(\"food_224/00000.jpg\"))\n",
    "image1 = image1[None]\n",
    "for i in range(5):\n",
    "    pred = deep_rank_model.predict([image1, image1, image1])[0]\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(10000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# dictionary with all the image names and their features\n",
    "features_dict = {}\n",
    "\n",
    "#images (warning this takes about 15GB of RAM)\n",
    "images = np.zeros((10000,224,224,3))\n",
    "\n",
    "# convert every image into features\n",
    "line_id = 0\n",
    "for filename in os.listdir('food_224'):\n",
    "    img = np.array(Image.open('food_224/' + filename))/255\n",
    "    images[line_id] = img\n",
    "    line_id += 1\n",
    "    if line_id % 10000 == 0:\n",
    "        print(line_id)\n",
    "        break\n",
    "    \n",
    "print(images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting 100\n",
      "predicting 200\n",
      "predicting 300\n",
      "predicting 400\n",
      "predicting 500\n",
      "predicting 600\n",
      "predicting 700\n",
      "predicting 800\n",
      "predicting 900\n",
      "predicting 1000\n",
      "predicting 1100\n",
      "predicting 1200\n",
      "predicting 1300\n",
      "predicting 1400\n",
      "predicting 1500\n",
      "predicting 1600\n",
      "predicting 1700\n",
      "predicting 1800\n",
      "predicting 1900\n",
      "predicting 2000\n",
      "predicting 2100\n",
      "predicting 2200\n",
      "predicting 2300\n",
      "predicting 2400\n",
      "predicting 2500\n",
      "predicting 2600\n",
      "predicting 2700\n",
      "predicting 2800\n",
      "predicting 2900\n",
      "predicting 3000\n",
      "predicting 3100\n",
      "predicting 3200\n",
      "predicting 3300\n",
      "predicting 3400\n",
      "predicting 3500\n",
      "predicting 3600\n",
      "predicting 3700\n",
      "predicting 3800\n",
      "predicting 3900\n",
      "predicting 4000\n",
      "predicting 4100\n",
      "predicting 4200\n",
      "predicting 4300\n",
      "predicting 4400\n",
      "predicting 4500\n",
      "predicting 4600\n",
      "predicting 4700\n",
      "predicting 4800\n",
      "predicting 4900\n",
      "predicting 5000\n",
      "predicting 5100\n",
      "predicting 5200\n",
      "predicting 5300\n",
      "predicting 5400\n",
      "predicting 5500\n",
      "predicting 5600\n",
      "predicting 5700\n",
      "predicting 5800\n",
      "predicting 5900\n",
      "predicting 6000\n",
      "predicting 6100\n",
      "predicting 6200\n",
      "predicting 6300\n",
      "predicting 6400\n",
      "predicting 6500\n",
      "predicting 6600\n",
      "predicting 6700\n",
      "predicting 6800\n",
      "predicting 6900\n",
      "predicting 7000\n",
      "predicting 7100\n",
      "predicting 7200\n",
      "predicting 7300\n",
      "predicting 7400\n",
      "predicting 7500\n",
      "predicting 7600\n",
      "predicting 7700\n",
      "predicting 7800\n",
      "predicting 7900\n",
      "predicting 8000\n",
      "predicting 8100\n",
      "predicting 8200\n",
      "predicting 8300\n",
      "predicting 8400\n",
      "predicting 8500\n",
      "predicting 8600\n",
      "predicting 8700\n",
      "predicting 8800\n",
      "predicting 8900\n",
      "predicting 9000\n",
      "predicting 9100\n",
      "predicting 9200\n",
      "predicting 9300\n",
      "predicting 9400\n",
      "predicting 9500\n",
      "predicting 9600\n",
      "predicting 9700\n",
      "predicting 9800\n",
      "predicting 9900\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "\n",
    "features = deep_rank_model.predict([images[0:size], images[0:size], images[0:size]])\n",
    "\n",
    "for i in np.arange(size,10001-size,size):\n",
    "    print(\"predicting\",i)\n",
    "    f = deep_rank_model.predict([images[i:i+size], images[i:i+size], images[i:i+size]])\n",
    "    features = np.concatenate([features, f])\n",
    "\n",
    "\n",
    "line_id = 0\n",
    "for filename in os.listdir('food_224'):\n",
    "    features_dict[filename.replace(\".jpg\",\"\")] = features[line_id]\n",
    "    line_id += 1\n",
    "    if line_id % 10000 == 0:\n",
    "        print(line_id)\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving features\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "print(\"saving features\")\n",
    "\n",
    "# save the features dictionary to a file using pickle\n",
    "with open('features.pickle', 'wb') as handle:\n",
    "    pickle.dump(features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "print(features_dict['00001'].shape)\n",
    "# print(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features_dict = {}\n",
    "# load the features dictionary from the file\n",
    "with open('features.pickle', 'rb') as handle:\n",
    "    features_dict = pickle.load(handle)\n",
    "    \n",
    "print(features_dict['00001'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = []\n",
    "line_id = 0\n",
    "with open('test_triplets.txt', 'r') as f:\n",
    "    # go through the lines\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "        embedding1 = features_dict[line.split(' ')[0]]\n",
    "        embedding2 = features_dict[line.split(' ')[1]]\n",
    "        embedding3 = features_dict[line.split(' ')[2]]\n",
    "\n",
    "        \n",
    "        #compare distance between embeddings\n",
    "        distance1 = np.sqrt(np.sum((embedding1 - embedding2)**2))\n",
    "        distance2 = np.sqrt(np.sum((embedding1 - embedding3)**2))\n",
    "\n",
    "        pred.append(1 if distance1 < distance2 else 0)\n",
    "\n",
    "        line_id += 1\n",
    "        if line_id % 1000 == 0:\n",
    "            print(line_id)\n",
    "            # break\n",
    "\n",
    "pred = np.array(pred,dtype=int)\n",
    "#save to file as int\n",
    "np.savetxt(\"submission.txt\", pred, delimiter=\"\\n\", fmt=\"%d\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
